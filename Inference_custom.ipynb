{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3df4d6c-cc26-4f69-b603-b517a5203ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = 'L:\\\\repos\\\\worldstrat\\\\ca-certificates.crt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b6b27-63a5-447b-aba2-4246dee458be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentinelhub import SHConfig, SentinelHubRequest, MimeType, CRS, BBox, DataCollection\n",
    "from eolearn.io import SentinelHubInputTask, SentinelHubEvalscriptTask\n",
    "from eolearn.core import EOTask, EOWorkflow, FeatureType, OutputTask, SaveTask, linearly_connect_tasks\n",
    "\n",
    "from torchvision.transforms import Compose, Resize, InterpolationMode, Normalize, Lambda\n",
    "from src.lightning_modules import LitModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from src.datasources import (\n",
    "    S2_ALL_12BANDS,\n",
    "    SN7_SUBDIRECTORIES,\n",
    "    S2_SN7_MEAN,\n",
    "    S2_SN7_STD,\n",
    "    SN7_BANDS_TO_READ,\n",
    "    SN7_MAX_EXPECTED_HR_VALUE,\n",
    "    SPOT_RGB_BANDS,\n",
    "    JIF_S2_MEAN,\n",
    "    JIF_S2_STD,\n",
    "    S2_ALL_BANDS,\n",
    "    SPOT_MAX_EXPECTED_VALUE_8_BIT,\n",
    "    SPOT_MAX_EXPECTED_VALUE_12_BIT,\n",
    "    ROOT_JIF_DATA_TRAIN,\n",
    "    METADATA_PATH,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Define your area of interest and time range\n",
    "bbox = BBox(bbox= [\n",
    "        -3.693118031193279,\n",
    "        40.403749979462816,\n",
    "        -3.6664122696536765,\n",
    "        40.42189244677638\n",
    "      ], crs=CRS.WGS84)\n",
    "time_interval = ('2020-03-12', '2020-06-13')\n",
    "\n",
    "config = SHConfig()\n",
    "config.sh_client_id = 'c93bbe9c-f393-4ac1-b862-70ca42f1be14'#getpass.getpass('Client Id')\n",
    "config.sh_client_secret = 'PqxiE6rWyrldjkr1yIjq0anJHvI6nLdu' #getpass.getpass('Client Secret')\n",
    "config.save()\n",
    "resolution=10\n",
    "cache_folder='data'\n",
    "\n",
    "# Create a SentinelHubRequest\n",
    "request = SentinelHubInputTask(\n",
    "    data_collection=DataCollection.SENTINEL2_L2A,\n",
    "    bands_feature=(FeatureType.DATA, \"L2A_data\"),\n",
    "    additional_data=[(FeatureType.MASK, \"dataMask\"),(FeatureType.MASK, \"CLM\"),(FeatureType.MASK, \"SCL\")],\n",
    "    resolution=resolution,\n",
    "    maxcc=1,\n",
    "    time_difference=dt.timedelta(hours=2),\n",
    "    cache_folder = cache_folder,\n",
    ")\n",
    "\n",
    "response = request.execute(bbox=bbox,time_interval=time_interval)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07262307-92cf-416d-8c09-11070469e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.data['L2A_data']\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52717ebb-7449-4f27-bd50-0f763ad525f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.transpose(data[27,:,:,[3,2,1]], (1,2,0)))\n",
    "\n",
    "#[1,3,7,9,21,22,23,27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f89aa7-2355-4f30-b63d-e134c14aea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = {}\n",
    "input_size=(160, 160)\n",
    "output_size=(1054, 1054)\n",
    "interpolation=InterpolationMode.BICUBIC\n",
    "normalize_lr=True\n",
    "scene_classification_to_color=False\n",
    "radiometry_depth=12\n",
    "\n",
    "lr_bands_to_use = np.array(S2_ALL_BANDS) - 1\n",
    "normalize = Normalize(\n",
    "            mean=JIF_S2_MEAN[lr_bands_to_use], std=JIF_S2_STD[lr_bands_to_use]\n",
    "        )\n",
    "transforms[\"lr\"] = Compose(\n",
    "        [\n",
    "            Lambda(lambda lr_revisit: torch.as_tensor(lr_revisit)),\n",
    "            normalize,\n",
    "            Resize(size=input_size, interpolation=interpolation, antialias=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "transforms[\"lrc\"] = Compose(\n",
    "    [\n",
    "        Lambda(\n",
    "            lambda lr_scene_classification: torch.as_tensor(lr_scene_classification)\n",
    "        ),\n",
    "        # Categorical\n",
    "        Resize(size=input_size, interpolation=InterpolationMode.NEAREST),\n",
    "        # Categorical to RGB; NOTE: interferes with FilterData\n",
    "        SceneClassificationToColorTransform\n",
    "        if scene_classification_to_color\n",
    "        else Compose([]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fc8bb6-be03-4b21-a50a-cad216613f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint, device):\n",
    "    \"\"\" Loads a model from a checkpoint.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint : str\n",
    "        Path to the checkpoint.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : lightning_modules.LitModel\n",
    "        The model.\n",
    "    \"\"\"    \n",
    "    model = LitModel.load_from_checkpoint(checkpoint).eval()\n",
    "    return model.to(device)\n",
    "\n",
    "def bias_adjust(y_hat, y):\n",
    "    \"\"\" Adjust the bias of the output of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_hat : torch.Tensor\n",
    "        The output of the model (super-resolved image).\n",
    "    y : torch.Tensor\n",
    "        The ground truth (high-resolution image).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_hat : torch.Tensor\n",
    "        The output of the model (super-resolved image) with bias adjusted.\n",
    "    \"\"\"    \n",
    "    b = (y - y_hat).mean(dim=(-1, -2), keepdim=True)\n",
    "    return y_hat + b\n",
    "\n",
    "def infer_chip(input_chip, model):\n",
    "    input_data = np.transpose(input_chip, (0, 3, 1, 2))\n",
    "    input_data = np.expand_dims(input_data, axis=0)  \n",
    "    input_tensor= torch.tensor(input_data, dtype=torch.float32)\n",
    "    input_tensor.shape\n",
    "    \n",
    "    # Ensure input_tensor has shape [batch_size, channels, height, width]\n",
    "    input_tensor = input_tensor.squeeze(0)  # Remove batch dimension temporarily\n",
    "    #input_tensor = input_tensor.transpose(0, 1)  # Change to [channels, bands, height, width]\n",
    "    #input_tensor = input_tensor.view(-1, input_tensor.shape[2], input_tensor.shape[3])  # Flatten to [channels*batches, height, width]\n",
    "    \n",
    "    # Apply the transformations\n",
    "    transformed_input = transforms[\"lr\"](input_tensor)\n",
    "    \n",
    "    # Now `transformed_input` contains the transformed data\n",
    "    #print(transformed_input.shape)  # Debug print to check the shape\n",
    "    \n",
    "    \n",
    "    # Reshape the output back to the original format if needed\n",
    "    transformed_input = transformed_input.view(1, 8, 12, *transformed_input.shape[2:])\n",
    "    transformed_input.shape\n",
    "\n",
    "    #transformed_input = np.expand_dims(transformed_input, axis=0) \n",
    "    y = model(transformed_input)#.detach().numpy()\n",
    "\n",
    "    np.shape(transformed_input)\n",
    "    output_tensor = F.interpolate(transformed_input[:,0,[3,2,1],:,:], size=( 156, 156), mode='bilinear', align_corners=False)\n",
    "    np.shape(output_tensor)\n",
    "    \n",
    "    b = (output_tensor - y).mean(dim=(-1, -2), keepdim=True)\n",
    "    y = bias_adjust(y, output_tensor).detach().numpy()\n",
    "    \n",
    "    y_numpy = np.squeeze(y).transpose(1, 2, 0)\n",
    "    return y_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c392c-3521-4456-a148-65e145c0dbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chips(image, chip_size):\n",
    "    \"\"\"\n",
    "    Extract non-overlapping chips of size chip_size from the input image.\n",
    "    Args:\n",
    "        image (numpy.ndarray): Input image of shape [bands, height, width, channels]\n",
    "        chip_size (tuple): Size of each chip (chip_height, chip_width)\n",
    "    Returns:\n",
    "        list: List of chips\n",
    "        list: List of positions of chips in the format (i, j)\n",
    "    \"\"\"\n",
    "    bands, height, width, channels = image.shape\n",
    "    chip_height, chip_width = chip_size\n",
    "    chips = []\n",
    "    positions = []\n",
    "\n",
    "    for i in range(0, height, chip_height):\n",
    "        for j in range(0, width, chip_width):\n",
    "            chip = np.full((bands, chip_height, chip_width, channels), np.nan)\n",
    "            actual_height = min(chip_height, height - i)\n",
    "            actual_width = min(chip_width, width - j)\n",
    "            chip[:, :actual_height, :actual_width, :] = image[:, i:i+actual_height, j:j+actual_width, :]\n",
    "            chips.append(chip)\n",
    "            positions.append((i, j))\n",
    "\n",
    "    return chips, positions\n",
    "\n",
    "def recompose_image(chips, positions, image_shape, chip_size, output_size):\n",
    "    \"\"\"\n",
    "    Recompose the super-resolved chips back into the full image.\n",
    "    Args:\n",
    "        chips (list): List of super-resolved chips\n",
    "        positions (list): List of positions of chips in the format (i, j)\n",
    "        image_shape (tuple): Original shape of the image (bands, height, width, channels)\n",
    "        chip_size (tuple): Size of each input chip (chip_height, chip_width)\n",
    "        output_size (tuple): Size of each super-resolved chip (output_height, output_width)\n",
    "    Returns:\n",
    "        numpy.ndarray: Recomposed super-resolved image\n",
    "    \"\"\"\n",
    "    _, original_height, original_width, original_channels = image_shape\n",
    "    chip_height, chip_width = chip_size\n",
    "    output_height, output_width = output_size\n",
    "\n",
    "    recomposed_height = (original_height // chip_height) * output_height\n",
    "    recomposed_width = (original_width // chip_width) * output_width\n",
    "    recomposed_image = np.zeros((recomposed_height, recomposed_width, np.shape(chips[0])[2]))\n",
    "    count_matrix = np.zeros((recomposed_height, recomposed_width, np.shape(chips[0])[2]))\n",
    "\n",
    "    for k, (i, j) in enumerate(positions):\n",
    "        chip = chips[k]\n",
    "        if np.shape(chip)[0] != output_height or np.shape(chip)[1] != output_width:\n",
    "            # Pad the chip to the required size if it's smaller\n",
    "            pad_height = output_height - chip.shape[0]\n",
    "            pad_width = output_width - chip.shape[1]\n",
    "            chip = np.pad(chip, ((0, pad_height), (0, pad_width), (0, 0)), mode='constant')\n",
    "        recomposed_image[i//chip_height*output_height:i//chip_height*output_height+output_height, \n",
    "                         j//chip_width*output_width:j//chip_width*output_width+output_width, :] += chip\n",
    "        count_matrix[i//chip_height*output_height:i//chip_height*output_height+output_height, \n",
    "                     j//chip_width*output_width:j//chip_width*output_width+output_width, :] += 1\n",
    "\n",
    "    recomposed_image /= count_matrix\n",
    "    return recomposed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeb0551-e4cd-4aaf-b6fe-e23da2a7ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(nparray, path):\n",
    "    from PIL import Image\n",
    "    \n",
    "    print(np.shape(nparray))\n",
    "    formatted = (nparray * 255 / np.max(nparray)).astype('uint8')\n",
    "    im = Image.fromarray(formatted)\n",
    "    im.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ecc4cc-44a4-4a3c-8b54-4b5c7889a39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Uncomment to use the CPU\n",
    "device = 'cpu'\n",
    "\n",
    "# Uncomment to use the GPU\n",
    "# device = 'cuda'\n",
    "\n",
    "#print('Using device:', device)\n",
    "#print()\n",
    "model = load_model('pretrained_model/model.ckpt', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe43fdb-cb41-42cc-a874-af0e3cbb6309",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = data[[1,3,7,9,21,22,23,27],:,:,:]\n",
    "chip_size = (20,20)\n",
    "output_size = (156,156)\n",
    "# Extract chips from the input image\n",
    "chips, positions =  extract_chips(input_data, chip_size)\n",
    "\n",
    "# Transform chips and apply the model\n",
    "transformed_chips = []\n",
    "for i,chip in enumerate(tqdm(chips)):\n",
    "    #fig, ax = plt.subplots(1,2, figsize=(150, 150))\n",
    "    \n",
    "    #chip = torch.tensor(chip, dtype=torch.float32)\n",
    "    #chip = chip.permute(3, 0, 1, 2)  # Change to [channels, bands, height, width]\n",
    "    #chip = chip.view(-1, chip.shape[2], chip.shape[3])  # Flatten to [channels*batches, height, width]\n",
    "    super_resolved_chip = infer_chip(chip, model) \n",
    "    #print(np.shape(super_resolved_chip))# Apply the model\n",
    "    #super_resolved_chip = super_resolved_chip.view(-1, 4, output_size[0], output_size[1])  # Reshape back\n",
    "    super_resolved_chip = [minmax_scale(band, feature_range=(0,0.5)) for band in super_resolved_chip]\n",
    "    transformed_chips.append(super_resolved_chip)  # Change to [bands, height, width, channels]\n",
    "    #ax[0].imshow(chip[0,:,:,:][:,:,[3,2,1]])\n",
    "    #ax[1].imshow(super_resolved_chip)\n",
    "    #plt.show()\n",
    "\n",
    "'''\n",
    "    save_image(chip[0,:,:,:][:,:,[3,2,1]],f'output/original_{i}.png')\n",
    "    save_image(super_resolved_chip,f'output/superresolved_{i}.png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7963990-2330-4111-848d-5b8b6392443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompose the super-resolved chips back into the full image\n",
    "#super_resolved_image = recompose_image(transformed_chips,  np.shape(input_data), input_size, output_size)\n",
    "\n",
    "#print(np.shape(super_resolved_image))  # Debug print to check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b86cdcb-2469-4ad5-ac43-fd7ac40e6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, original_height, original_width, original_channels = np.shape(input_data)\n",
    "chip_height, chip_width = chip_size\n",
    "output_height, output_width = output_size\n",
    "recomposed_height = (original_height // chip_height) * output_height\n",
    "recomposed_width = (original_width // chip_width) * output_width\n",
    "recomposed_image = np.zeros((recomposed_height, recomposed_width, np.shape(transformed_chips[0])[2]))\n",
    "count_matrix = np.zeros((recomposed_height, recomposed_width, np.shape(transformed_chips[0])[2]))\n",
    "\n",
    "\n",
    "k = 0\n",
    "for i in range(0, recomposed_height, output_height):\n",
    "    for j in range(0, recomposed_width, output_width):\n",
    "        if k < len(transformed_chips):\n",
    "            chip = transformed_chips[k]\n",
    "            recomposed_image[i:i+output_height, j:j+output_width, :] += chip\n",
    "            count_matrix[i:i+output_height, j:j+output_width, :] += 1\n",
    "            k += 1\n",
    "\n",
    "recomposed_image /= count_matrix\n",
    "super_resolved_image = recomposed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a85b43-75d4-427a-83a1-286b4fad56b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(super_resolved_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52de77dc-8f6f-4059-8558-646cb586eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "formatted = (super_resolved_image * 255 / np.max(super_resolved_image)).astype('uint8')\n",
    "print(np.shape(formatted))\n",
    "cv2.imwrite(\"output/super_resolved_image.jpeg\", formatted)\n",
    "\n",
    "scaled_out = [minmax_scale(band, feature_range=(0,0.5)) for band in super_resolved_image]\n",
    "formatted = (scaled_out * 255 / np.max(scaled_out)).astype('uint8')\n",
    "print(np.shape(formatted))\n",
    "cv2.imwrite(\"output/scaled_super_resolved_image.jpeg\", formatted)\n",
    "\n",
    "indata = input_data[0,:,:,:][:,:,[3,2,1]]\n",
    "formatted = (indata * 255 / np.max(indata)).astype('uint8')\n",
    "print(np.shape(formatted))\n",
    "cv2.imwrite(\"output/original.jpeg\", formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a392e-29b9-49e4-b885-12379c9dc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "y_res = np.squeeze(super_resolved_image)\n",
    "y_res = [minmax_scale(band, feature_range=(0,0.5)) for band in y_res]\n",
    "x_res = indata\n",
    "x_res = [minmax_scale(band, feature_range=(0,0.5)) for band in x_res]\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(150, 150))\n",
    "ax[0].imshow(x_res)\n",
    "ax[1].imshow(y_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ce31e-3dd0-43d7-915b-44d3f0cb47ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba139e5b-26ac-4319-bf26-f140a85cc2e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08739efa-7180-49b1-9341-5bc48a1c3423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa686be8-06e3-4e4e-8ef0-ec191d65675a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559f661a-346a-42ab-9af9-e16af829aa27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cf229f-d493-40d6-838b-8abe91b75539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241de993-cb01-4452-aee3-4354ff7b7eba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8542b34-5b8c-44ef-9c87-7a7f3ff306a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
